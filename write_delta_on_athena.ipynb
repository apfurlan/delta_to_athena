{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apfurlan/delta_to_athena/blob/main/write_delta_on_athena.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb6d2f0-e576-46b9-92b9-caab4e34f22a",
      "metadata": {
        "id": "ddb6d2f0-e576-46b9-92b9-caab4e34f22a"
      },
      "source": [
        "# **Write Delta Tables on Athena**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97382515-6599-46fd-b487-67e28af423b2",
      "metadata": {
        "id": "97382515-6599-46fd-b487-67e28af423b2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from boto.s3.connection import S3Connection\n",
        "import time, boto3\n",
        "from delta import *\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47eb1c61-57d1-41fb-a733-df7816292851",
      "metadata": {
        "id": "47eb1c61-57d1-41fb-a733-df7816292851"
      },
      "source": [
        "## **Configure Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bee7ace-d66f-4185-a4bf-74ce98bc0093",
      "metadata": {
        "id": "8bee7ace-d66f-4185-a4bf-74ce98bc0093"
      },
      "outputs": [],
      "source": [
        "builder = (\n",
        "    SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.jars.packages\", [\"io.delta:delta-core_2.12:1.0.0\",\"org.apache.hadoop:hadoop-aws:3.2.0\"])\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.hadoop.fs.s3a.fast.upload\", True)\n",
        ")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0339a4b9-d678-40bf-9f78-3925b55553f6",
      "metadata": {
        "id": "0339a4b9-d678-40bf-9f78-3925b55553f6"
      },
      "source": [
        "## **Generate Manifest to connect to Athena**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a8d43a-e04e-4214-8d48-13884c4e9993",
      "metadata": {
        "id": "f2a8d43a-e04e-4214-8d48-13884c4e9993"
      },
      "outputs": [],
      "source": [
        "athena = boto3.client(\n",
        "    \"athena\",\n",
        "    region_name=\"us-east-1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc01ee3-ab5b-4c5e-be64-70250f5b989a",
      "metadata": {
        "id": "dbc01ee3-ab5b-4c5e-be64-70250f5b989a"
      },
      "outputs": [],
      "source": [
        "def build_hive_ddl(\n",
        "        table_name, object_schema, symlink_location, partition_cols=[], verbose=False):\n",
        "    \"\"\"\n",
        "    :param table_name: the name of the table you want to register in the Hive metastore    \n",
        "    :param object_schema: an instance of pyspark.sql.Dataframe.schema\n",
        "    :param location: the storage location for this data (and S3 or HDFS filepath)\n",
        "    :param partition_schema: an optional instance of pyspark.sql.Dataframe.schema that stores the\n",
        "    columns that are used for partitioning on disk\n",
        "    :param verbose:\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    columns = (\n",
        "        ','.join(\n",
        "            [field.simpleString() for field in object_schema if field.name not in partition_cols]\n",
        "        )\n",
        "    ).replace(':', ' ')\n",
        "    \n",
        "    partition_schema = (\n",
        "    ','.join(\n",
        "        [field.simpleString() for field in object_schema if field.name in partition_cols]\n",
        "        )\n",
        "    ).replace(':', ' ')\n",
        "    \n",
        "    ddl = 'CREATE EXTERNAL TABLE '+table_name+' ('\\\n",
        "        + columns + ')'\\\n",
        "        + (\n",
        "              f\" PARTITIONED BY ({partition_schema}) \"\n",
        "              if partition_schema else ''\n",
        "          )\\\n",
        "        + \" ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \" \\\n",
        "        + \" STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\" \\\n",
        "        + \" OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\" \\\n",
        "        + f\" LOCATION '{symlink_location}'\"\n",
        "    if verbose:\n",
        "        print('Generated Hive DDL:\\n'+ddl)\n",
        "    return ddl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebf2004-f428-4884-be23-1579e2e3fa9b",
      "metadata": {
        "id": "cebf2004-f428-4884-be23-1579e2e3fa9b"
      },
      "outputs": [],
      "source": [
        "def run_athena_query(db_name, workgroup, query):\n",
        "\n",
        "    response = athena.start_query_execution(\n",
        "        QueryString=query,\n",
        "        QueryExecutionContext={\n",
        "            \"Database\": db_name,\n",
        "            \"Catalog\": \"AwsDataCatalog\",\n",
        "        },\n",
        "        ResultConfiguration={\n",
        "            'OutputLocation': 's3://mater-dei-dl-562415927517/athena_output/'\n",
        "        },\n",
        "        WorkGroup=workgroup\n",
        "    )\n",
        "\n",
        "    timout = 60\n",
        "    while timout > 0:\n",
        "        time.sleep(0.1)\n",
        "        timout -= 0.1\n",
        "        result = athena.get_query_execution(\n",
        "            QueryExecutionId=response[\"QueryExecutionId\"]\n",
        "        )\n",
        "        if result[\"QueryExecution\"][\"Status\"][\"State\"] == \"SUCCEEDED\":\n",
        "            break\n",
        "\n",
        "    if result[\"QueryExecution\"][\"Status\"][\"State\"] == \"SUCCEEDED\":\n",
        "        output = athena.get_query_results(QueryExecutionId=response[\"QueryExecutionId\"])\n",
        "        return output\n",
        "    else:\n",
        "        print(result)\n",
        "        raise \"Not possible to run athena query\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace73098-ad87-4212-ba36-b47f13db8918",
      "metadata": {
        "tags": [],
        "id": "ace73098-ad87-4212-ba36-b47f13db8918"
      },
      "outputs": [],
      "source": [
        "def add_delta_2_athena(bucket_name, delta_path, db_name, workgroup, table_name, partition_cols = []):\n",
        "    if len(\n",
        "            run_athena_query(db_name,workgroup,f\"SHOW TABLES LIKE '{table_name}';\")['ResultSet']['Rows']\n",
        "        ) > 0:\n",
        "        print('Table already existis!')\n",
        "        return\n",
        "    else:\n",
        "        \n",
        "        delta_df = DeltaTable.forPath(\n",
        "            spark, f's3a://{bucket_name}/{delta_path}'\n",
        "        ).toDF()\n",
        "            \n",
        "        ddl = build_hive_ddl(\n",
        "            f'{db_name}.{table_name}',\n",
        "            delta_df.schema,\n",
        "            f's3://{bucket_name}/{delta_path}/_symlink_format_manifest/',\n",
        "            partition_cols\n",
        "        )\n",
        "        run_athena_query(db_name,workgroup,ddl)\n",
        "        # Repair Table\n",
        "        run_athena_query(db_name,workgroup,f\"MSCK REPAIR TABLE {table_name};\")\n",
        "        print('SUCCESS !!!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d87a43-c7ba-4faf-ba9a-ea803f0d9364",
      "metadata": {
        "id": "39d87a43-c7ba-4faf-ba9a-ea803f0d9364"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'bucket_name'\n",
        "dbname = 'dbname_on_glue'\n",
        "workgroup = 'primary'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6923db40-92c4-4b9a-8206-fff14e113227",
      "metadata": {
        "tags": [],
        "id": "6923db40-92c4-4b9a-8206-fff14e113227",
        "outputId": "9a972fc3-007c-4556-f8e4-7025326ea550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS !!!!\n"
          ]
        }
      ],
      "source": [
        "full_load_path = bucket_name+'/path'\n",
        "\n",
        "table_path = 'path_on_s3/folder_name'\n",
        "table_name = 'folder_name'\n",
        "\n",
        "stagingData = DeltaTable.forPath(spark, f's3a://{bucket_name}/{table_path}')\n",
        "stagingData.generate(\"symlink_format_manifest\")\n",
        "add_delta_2_athena(bucket_name, table_path, dbname, workgroup, table_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "write_delta_on_athena.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}